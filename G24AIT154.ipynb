{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMroRHw2GD9y6GNYHZAhhfN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsvinay9/Assignment0/blob/main/G24AIT154.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "c-kLqrIRgwbD",
        "outputId": "08b484b0-7727-4f02-8d13-d2d4b168bc7a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'hmmlearn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-77a9e7dc5698>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhmmlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# PoS Tagger Using HMMs - Based on Provided Inline Data (Simulating Penn Treebank-like Structure)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hmmlearn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import hmmlearn\n",
        "\n",
        "# PoS Tagger Using HMMs - Based on Provided Inline Data (Simulating Penn Treebank-like Structure)\n",
        "\n",
        "# Inline dataset for testing\n",
        "# Format: [\"sentence string\", [list of POS tags]]\n",
        "data = [\n",
        "    [\"the cat sat\", [\"DET\", \"NOUN\", \"VERB\"]],\n",
        "    [\"the dog barked\", [\"DET\", \"NOUN\", \"VERB\"]],\n",
        "    [\"a dog sat\", [\"DET\", \"NOUN\", \"VERB\"]],\n",
        "    [\"the dog ran\", [\"DET\", \"NOUN\", \"VERB\"]],\n",
        "    [\"a cat barked\", [\"DET\", \"NOUN\", \"VERB\"]],\n",
        "    [\"a dog barked\", [\"DET\", \"NOUN\", \"VERB\"]],  # Additional test case\n",
        "    [\"the cat ran\", [\"DET\", \"NOUN\", \"VERB\"]]     # Additional test case\n",
        "]\n",
        "\n",
        "# Split into sentences and tags\n",
        "sentences = [entry[0].split() for entry in data]\n",
        "tags = [entry[1] for entry in data]\n",
        "\n",
        "# Train/Test Split\n",
        "train_size = int(0.8 * len(sentences))\n",
        "train_sentences, test_sentences = sentences[:train_size], sentences[train_size:]\n",
        "train_tags, test_tags = tags[:train_size], tags[train_size:]\n",
        "\n",
        "# Count tag and word-tag frequencies\n",
        "transition_counts = defaultdict(lambda: defaultdict(int))\n",
        "transition2_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "emission_counts = defaultdict(lambda: defaultdict(int))\n",
        "context_counts = defaultdict(int)\n",
        "word_tag_prevword_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "\n",
        "for sent, tags in zip(train_sentences, train_tags):\n",
        "    prev_tag = \"<s>\"\n",
        "    prev_word = \"<s>\"\n",
        "    for i in range(len(sent)):\n",
        "        word = sent[i]\n",
        "        tag = tags[i]\n",
        "        context_counts[tag] += 1\n",
        "        emission_counts[tag][word] += 1\n",
        "        transition_counts[prev_tag][tag] += 1\n",
        "        word_tag_prevword_counts[prev_word][tag][word] += 1\n",
        "        if i > 0:\n",
        "            prev_prev_tag = tags[i-2] if i > 1 else \"<s>\"\n",
        "            transition2_counts[prev_prev_tag][tags[i-1]][tag] += 1\n",
        "        prev_tag = tag\n",
        "        prev_word = word\n",
        "\n",
        "tag_set = list(context_counts.keys())\n",
        "\n",
        "# Smoothing function\n",
        "def smoothed_prob(numerator, denominator):\n",
        "    return (numerator + 1) / (denominator + len(context_counts))\n",
        "\n",
        "# First Order HMM (P(word | tag))\n",
        "def predict_first_order(sent):\n",
        "    prediction = []\n",
        "    for word in sent:\n",
        "        best_tag = max(tag_set, key=lambda tag: smoothed_prob(emission_counts[tag][word], context_counts[tag]))\n",
        "        prediction.append(best_tag)\n",
        "    return prediction\n",
        "\n",
        "# Second Order HMM (P(tag | prev_tag) and Viterbi)\n",
        "def predict_second_order(sent):\n",
        "    V = [{}]\n",
        "    path = {}\n",
        "\n",
        "    for tag in tag_set:\n",
        "        V[0][tag] = smoothed_prob(transition_counts['<s>'][tag], sum(transition_counts['<s>'].values())) * \\\n",
        "                    smoothed_prob(emission_counts[tag][sent[0]], context_counts[tag])\n",
        "        path[tag] = [tag]\n",
        "\n",
        "    for i in range(1, len(sent)):\n",
        "        V.append({})\n",
        "        new_path = {}\n",
        "        for curr_tag in tag_set:\n",
        "            (prob, prev_tag) = max((V[i-1][pt] * smoothed_prob(transition_counts[pt][curr_tag], context_counts[pt]) * \\\n",
        "                                      smoothed_prob(emission_counts[curr_tag][sent[i]], context_counts[curr_tag]), pt)\n",
        "                                     for pt in tag_set)\n",
        "            V[i][curr_tag] = prob\n",
        "            new_path[curr_tag] = path[prev_tag] + [curr_tag]\n",
        "        path = new_path\n",
        "\n",
        "    final_tag = max(V[-1], key=V[-1].get)\n",
        "    return path[final_tag]\n",
        "\n",
        "# First Order HMM with Previous Word (P(word | tag, prev_word))\n",
        "def predict_with_prev_word(sent):\n",
        "    prediction = []\n",
        "    prev_word = \"<s>\"\n",
        "    for word in sent:\n",
        "        best_tag = max(tag_set, key=lambda tag: smoothed_prob(word_tag_prevword_counts[prev_word][tag][word], context_counts[tag]))\n",
        "        prediction.append(best_tag)\n",
        "        prev_word = word\n",
        "    return prediction\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(predict_func):\n",
        "    correct = total = 0\n",
        "    for sent, true_tags in zip(test_sentences, test_tags):\n",
        "        pred_tags = predict_func(sent)\n",
        "        for pt, tt in zip(pred_tags, true_tags):\n",
        "            if pt == tt:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total if total > 0 else 0\n",
        "\n",
        "# Run Evaluations\n",
        "print(\"First Order HMM Accuracy:\", evaluate(predict_first_order))\n",
        "print(\"Second Order HMM Accuracy:\", evaluate(predict_second_order))\n",
        "print(\"First Order + Prev Word Accuracy:\", evaluate(predict_with_prev_word))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m69o8qkPhHAM",
        "outputId": "e5c54eca-33ff-48d4-c2b2-4dd804a5a83a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Order HMM Accuracy: 1.0\n",
            "Second Order HMM Accuracy: 1.0\n",
            "First Order + Prev Word Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sgr2Lv8g8lQ",
        "outputId": "ed022d44-2f11-4489-e703-ff8b87fa12eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n",
            "Downloading hmmlearn-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.9/165.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.3\n"
          ]
        }
      ]
    }
  ]
}